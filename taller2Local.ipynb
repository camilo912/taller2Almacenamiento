{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, csv, glob, json, uuid, pickle, math\n",
    "import nltk \n",
    "import gensim, logging\n",
    "import numpy as np, scipy, pandas as pd\n",
    "from operator import itemgetter\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_INDEX = 9\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "CONTENT_PATH = './inputs/contents/'\n",
    "TOKENS_PATH = './inputs/tokens/'\n",
    "CENTROIDS_PATH = './inputs/centroids/'\n",
    "BM25_PATH = './inputs/bm25/'\n",
    "MODEL_PATH = './model/'\n",
    "\n",
    "if not os.path.exists(CONTENT_PATH):\n",
    "    os.makedirs(CONTENT_PATH)\n",
    "    \n",
    "if not os.path.exists(TOKENS_PATH):\n",
    "    os.makedirs(TOKENS_PATH)\n",
    "    \n",
    "if not os.path.exists(CENTROIDS_PATH):\n",
    "    os.makedirs(CENTROIDS_PATH)\n",
    "\n",
    "if not os.path.exists(BM25_PATH):\n",
    "    os.makedirs(BM25_PATH)\n",
    "    \n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for fname in glob.iglob('./inputs/*.csv', recursive=False):\n",
    "    f = open(fname)\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        count = count + 1\n",
    "        content = line[CONTENT_INDEX]\n",
    "        cname = CONTENT_PATH + str(count) + '.txt'\n",
    "        tname = TOKENS_PATH + str(count) + '.tokens'\n",
    "        cf = open(cname, 'w')\n",
    "        cf.write(content)\n",
    "        cf.close()\n",
    "        tf = open(tname, 'w')\n",
    "        for sentence in nltk.sent_tokenize(content):\n",
    "            tf.write(\"%s\\n\" % sentence.lower())\n",
    "        tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:11, 87.84it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for fname in tqdm(glob.iglob('./inputs/tokens/*.tokens')):\n",
    "    for line in open(fname, 'r'):\n",
    "        sentences.append(nltk.word_tokenize(line.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=1, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model/w2v-lc.model')\n",
    "model.wv.save_word2vec_format('./model/w2v-lc.model.bin', binary=True)\n",
    "vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "with open('./model/w2v-lc-vocab.json', 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congressional', 0.8006001710891724),\n",
       " ('parliament', 0.7711571455001831),\n",
       " ('enforcing', 0.7637171745300293),\n",
       " ('committee', 0.7455602884292603),\n",
       " ('military', 0.7395405769348145),\n",
       " ('committees', 0.7362181544303894),\n",
       " ('law', 0.7357364892959595),\n",
       " ('judiciary', 0.7344596982002258),\n",
       " ('union', 0.7311062812805176),\n",
       " ('filibuster', 0.7302385568618774)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['texas', 'senate'], negative=['alabama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(x, out=False):\n",
    "    if x in model.wv.vocab:\n",
    "        if out == True:\n",
    "            return model.syn1neg[model.wv.vocab[x].index]\n",
    "        else:\n",
    "            return model[x]\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for fname in glob.iglob('./inputs/contents/*.txt', recursive=False):\n",
    "    for line in open(fname):\n",
    "        centroid_in = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(line.lower())]), axis=0))\n",
    "        centroid_out = (np.mean(np.array([get_embedding(x, out=True) for x in nltk.word_tokenize(line.lower())]), axis=0))\n",
    "        out_dict = { fname : (centroid_in, centroid_out) }\n",
    "        pickle_file = './inputs/centroids/' + os.path.basename(fname).replace('.txt', '.p')\n",
    "        pickle.dump(out_dict, open(pickle_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document(q_embeddings, d_centroid):\n",
    "    individual_csims = [(1 - scipy.spatial.distance.cosine(qin, d_centroid)) for qin in q_embeddings]\n",
    "    return (sum(individual_csims)/len(q_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Sentences(object):\n",
    "    def __init__(self, pattern):\n",
    "        self.pattern = pattern\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in glob.iglob(self.pattern, recursive=True):\n",
    "            for line in open(fname):\n",
    "                yield nltk.word_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(38818 unique tokens: ['content', '.', ':', 'a', 'administration']...)\n"
     ]
    }
   ],
   "source": [
    "sentences = BM25Sentences('./inputs/tokens/*.tokens')\n",
    "dictionary = gensim.corpora.Dictionary(line for line in sentences)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('./inputs/bm25/allnews.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25dict = dictionary.load('./inputs/bm25/allnews.dict') \n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in glob.iglob(self.dirname + '*.tokens'):\n",
    "            doc = open(fname).read().replace('\\n', '')\n",
    "            yield bm25dict.doc2bow(nltk.word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "citer = MyCorpus(TOKENS_PATH)\n",
    "corpus = [x for x in citer]\n",
    "print (len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.corpora.MmCorpus.serialize('./inputs/bm25/allnewscorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25corpus = gensim.corpora.MmCorpus('./inputs/bm25/allnewscorpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('./model/w2v-lc.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_dict = {}\n",
    "for fname in glob.iglob('./inputs/centroids/*.p', recursive=False):\n",
    "    centroid_dict.update(pickle.load(open(fname, \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_centroid_dict = {k: centroid_dict[k] for k in centroid_dict if not np.isnan(centroid_dict[k][0]).any()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25dict = gensim.corpora.Dictionary().load('./inputs/bm25/allnews.dict') \n",
    "bm25corpus = gensim.corpora.MmCorpus('./inputs/bm25/allnewscorpus.mm')\n",
    "bm25 = gensim.summarization.bm25.BM25(bm25corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'political stability and economic health'\n",
    "query_words = nltk.word_tokenize(query.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bm25.get_scores(bm25dict.doc2bow(query_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./inputs/contents/169.txt\n",
      "./inputs/contents/108.txt\n",
      "./inputs/contents/8.txt\n",
      "./inputs/contents/407.txt\n",
      "./inputs/contents/875.txt\n"
     ]
    }
   ],
   "source": [
    "best_result = ['./inputs/contents/'+str(x+1)+'.txt' for x in (sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5])]\n",
    "for fname in best_result:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in query:  5 Num query word in vectors:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "query_ins = [get_embedding(x) for x in query_words]\n",
    "q_len = len(query_ins)\n",
    "print('Num words in query: ', len(query_words), 'Num query word in vectors: ', q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_in_in = []\n",
    "scores_in_out = []\n",
    "for k,v in clean_centroid_dict.items():\n",
    "    scores_in_in.append((k, score_document(query_ins, v[0])))\n",
    "    scores_in_out.append((k, score_document(query_ins, v[1])))\n",
    "\n",
    "scores_in_in = sorted(scores_in_in, key=itemgetter(1), reverse=True)\n",
    "scores_in_out = sorted(scores_in_out, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 IN-IN:\n",
      "./inputs/contents/515.txt\n",
      "./inputs/contents/60.txt\n",
      "./inputs/contents/93.txt\n",
      "./inputs/contents/159.txt\n",
      "./inputs/contents/355.txt\n"
     ]
    }
   ],
   "source": [
    "print('TOP 5 IN-IN:')\n",
    "top_5_in_in = [x[0] for x in scores_in_in[:5]]\n",
    "\n",
    "for fname in top_5_in_in:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 IN-OUT:\n",
      "./inputs/contents/1.txt\n",
      "./inputs/contents/93.txt\n",
      "./inputs/contents/301.txt\n",
      "./inputs/contents/250.txt\n",
      "./inputs/contents/60.txt\n"
     ]
    }
   ],
   "source": [
    "print('TOP 5 IN-OUT:')\n",
    "top_5_in_out = [x[0] for x in scores_in_out[:5]]\n",
    "\n",
    "for fname in top_5_in_out:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>BM25       </td><td style=\"text-align: right;\"> 30931</td><td style=\"text-align: right;\"> 40023</td><td style=\"text-align: right;\">71852</td><td style=\"text-align: right;\">133532</td><td style=\"text-align: right;\">  1620</td></tr>\n",
       "<tr><td>DESM-IN-IN </td><td style=\"text-align: right;\">140797</td><td style=\"text-align: right;\"> 32221</td><td style=\"text-align: right;\">31472</td><td style=\"text-align: right;\"> 39594</td><td style=\"text-align: right;\">135444</td></tr>\n",
       "<tr><td>DESM-IN-OUT</td><td style=\"text-align: right;\"> 73280</td><td style=\"text-align: right;\">140797</td><td style=\"text-align: right;\">32221</td><td style=\"text-align: right;\"> 42404</td><td style=\"text-align: right;\"> 42105</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"BM25\",30931, 40023, 71852, 133532, 1620],\n",
    "         [\"DESM-IN-IN\", 140797, 32221, 31472, 39594, 135444],\n",
    "         [\"DESM-IN-OUT\", 73280, 140797, 32221, 42404, 42105]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
